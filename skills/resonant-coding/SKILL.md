---
name: resonant-coding
description: MetodologÃ­a completa de Resonant Coding - Regla de los 5, baldes limpios, tres expertos
scope: root
metadata:
  auto_invoke:
    - "resonant"
    - "regla de los 5"
    - "rule of five"
    - "baldes limpios"
    - "revisar"
    - "refinar"
    - "mejorar calidad"
    - "review completo"
allowed_tools: [read, write, memory_search, exec]
---

# Resonant Coding

MetodologÃ­a para trabajar efectivamente con LLMs evitando el caos y maximizando la calidad.

## ğŸ¯ Problema que resuelve

Usar IA para "acelerar" suele generar:
- âŒ CÃ³digo difÃ­cil de entender
- âŒ Calidad inconsistente
- âŒ Revisiones interminables
- âŒ **Resultado:** MÃS LENTO, no mÃ¡s rÃ¡pido

**Causa raÃ­z:** No entender cÃ³mo funcionan los LLMs.

## ğŸ§  CÃ³mo funcionan los LLMs

### 4 verdades fundamentales

1. **Son predictores de texto**
   - Como autocompletado del celular, pero masivo
   - No "piensan", predicen siguiente token probable

2. **No tienen memoria real**
   - Cada conversaciÃ³n es "nueva"
   - La "memoria" es pegar contexto anterior

3. **AtenciÃ³n limitada**
   - Textos largos â†’ peor resultado
   - Capacidad de contexto finita

4. **No son deterministas**
   - Misma pregunta â†’ diferentes respuestas
   - Temperatura > 0 = variabilidad

### La metÃ¡fora del balde

**Lavando platos en el rÃ­o con un balde:**

| Platos | Estado del agua | Resultado |
|--------|-----------------|-----------|
| 1 | Limpia, sobra | âœ… Ã“ptimo |
| 10 | Turbia | âš ï¸ Cuidado |
| 100 | Muy sucia | âŒ Empeoran |
| Algo grasoso | Inutilizable | ğŸ’€ Destruido |

**En IA:**
- ConversaciÃ³n corta + enfocada = balde limpio = respuesta excelente
- ConversaciÃ³n larga + desordenada = agua sucia = basura

**Resonancia** = Info que das â‰ˆ Capacidad de atenciÃ³n del modelo

## ğŸ“ Regla de los 5

Proceso iterativo de refinamiento en 5 filtros sucesivos.

### Los 5 filtros

#### 1ï¸âƒ£ Borrador
**Objetivo:** Crear contenido inicial completo

```
Pregunta clave: Â¿EstÃ¡ TODO lo necesario?

âœ… Preferir amplitud a profundidad
âœ… No importa que sea perfecto
âŒ No omitir partes "obvias"
```

**Ejemplo:**
```
âŒ Malo: "Agregar autenticaciÃ³n"

âœ… Bueno: 
- Instalar dependencias (jwt, bcrypt)
- Endpoint /login
- Middleware auth
- Rutas protegidas /dashboard
- Tests
```

#### 2ï¸âƒ£ CorrecciÃ³n
**Objetivo:** Arreglar errores e inconsistencias

```
Pregunta clave: Â¿Es CORRECTO?

âœ… Verificar datos/hechos
âœ… Chequear lÃ³gica
âœ… Buscar contradicciones
âŒ No asumir que el modelo "sabe"
```

**Ejemplo:**
```
Borrador: "Instalar express-jwt"

CorrecciÃ³n: 
âš ï¸  express-jwt estÃ¡ deprecated
âœ… Usar jsonwebtoken directamente
```

#### 3ï¸âƒ£ Claridad
**Objetivo:** Simplificar y hacer entendible

```
Pregunta clave: Â¿Se entiende a la PRIMERA?

âœ… Eliminar jerga innecesaria
âœ… Explicar conceptos complejos
âœ… Estructura lÃ³gica clara
âŒ No asumir conocimiento previo
```

**Ejemplo:**
```
âŒ Turbio:
"Implementar JWT stateless con RS256"

âœ… Claro:
"Sistema de autenticaciÃ³n con tokens:
- Token = pase temporal (expira en 1h)
- Usuario lo envÃ­a en cada request
- Servidor verifica sin consultar DB (stateless)"
```

#### 4ï¸âƒ£ Casos LÃ­mite
**Objetivo:** Identificar quÃ© podrÃ­a salir mal

```
Pregunta clave: Â¿QuÃ© pasa si...?

âœ… Inputs invÃ¡lidos
âœ… Condiciones inesperadas
âœ… Estados edge
âŒ No asumir "camino feliz"
```

**Ejemplo:**
```
AutenticaciÃ³n JWT:

Casos lÃ­mite:
- Â¿Token alterado? â†’ ValidaciÃ³n con secret
- Â¿Token expirado? â†’ Refresh token
- Â¿Usuario cambia password? â†’ Invalidar tokens
- Â¿Logout? â†’ Blacklist
- Â¿Rate limiting? â†’ 3 intentos/min
```

#### 5ï¸âƒ£ Excelencia
**Objetivo:** Optimizar y pulir

```
Pregunta clave: Â¿Es lo MEJOR que puede ser?

âœ… Performance
âœ… Mantenibilidad
âœ… DocumentaciÃ³n
âœ… Tests
âŒ No conformarse con "aceptable"
```

**Ejemplo:**
```
Mejoras:
- Variables de entorno para secrets âœ…
- Tests unitarios + integration âœ…
- Rate limiting en endpoints crÃ­ticos âœ…
- Logging de intentos fallidos âœ…
- DocumentaciÃ³n en README âœ…
```

### Orden flexible

**NO es secuencial rÃ­gido:**
- A veces necesitas mÃ¡s Claridad que CorrecciÃ³n
- Algunos documentos requieren 3 iteraciones de Casos LÃ­mite
- Usa como guÃ­a, no como ley

### AplicaciÃ³n iterativa

```
Borrador v1 â†’ CorrecciÃ³n â†’ Borrador v2 â†’ Claridad â†’ ...

El punto es tener ESTRUCTURA, no seguir ciegamente.
```

## ğŸ­ Los Tres Expertos

Dividir el trabajo en 3 conversaciones independientes (baldes limpios).

### El Investigador (ConversaciÃ³n 1)

**Objetivo:** Entender el problema

```markdown
Contexto limpio: SOLO sobre investigaciÃ³n

Tareas:
1. Leer lo que existe
2. Identificar partes clave
3. Sintetizar informaciÃ³n
4. Proponer opciones

Aplicar Regla de los 5 al resumen
```

**Prompt template:**

```
ActuÃ¡ como experto en [TEMA].

InvestigÃ¡:
1. Â¿QuÃ© existe actualmente en el proyecto?
2. Â¿QuÃ© soluciones hay disponibles?
3. Â¿CuÃ¡les son las opciones?
4. Â¿Ventajas/desventajas de cada una?

GenerÃ¡ un resumen estructurado.
```

**Output esperado:**
- Documento de investigaciÃ³n (2-5 pÃ¡ginas)
- Opciones evaluadas
- RecomendaciÃ³n justificada

**âš ï¸ CRÃTICO:** RevisiÃ³n humana exhaustiva
- El modelo NO sabe conocimiento implÃ­cito del equipo
- Validar recomendaciones con experiencia real

### La Estratega (ConversaciÃ³n 2)

**Objetivo:** Planificar paso a paso

```markdown
Contexto limpio: Resumen investigaciÃ³n + objetivo

Tareas:
1. Dividir en tareas PEQUEÃ‘AS
2. Identificar dependencias
3. Estimar complejidad
4. Detectar riesgos

Aplicar Regla de los 5 al plan
```

**Prompt template:**

```
Con base en esta investigaciÃ³n:
[PEGAR RESUMEN]

Genera plan paso a paso donde:
- Cada tarea cabe en un "balde" (< 4h)
- Tareas son independientes si es posible
- Dependencias estÃ¡n claras
- Riesgos identificados
```

**ValidaciÃ³n de tareas:**

```
âŒ Tarea muy grande:
"Implementar sistema de autenticaciÃ³n completo"

âœ… Tareas del tamaÃ±o correcto:
1. "Crear schema de usuarios en DB" (1h)
2. "Endpoint POST /register con validaciÃ³n" (2h)
3. "Endpoint POST /login que retorna JWT" (2h)
4. "Middleware de autenticaciÃ³n" (1h)
5. "Tests de endpoints auth" (2h)
```

**Output esperado:**
- Plan detallado con tareas numeradas
- Estimaciones realistas
- Mapa de dependencias
- Riesgos identificados

### El Ejecutor (Conversaciones 3+)

**Objetivo:** Implementar cada tarea

```markdown
Una conversaciÃ³n por tarea (balde limpio)

Contexto: Plan completo + tarea especÃ­fica

Tareas:
1. Implementar segÃºn plan
2. Validar resultado
3. Documentar cambios

Aplicar Regla de los 5 al cÃ³digo
```

**Prompt template:**

```
ImplementÃ¡ esta tarea del plan:

Tarea #[N]: [DESCRIPCIÃ“N]

Contexto necesario:
[PEGAR SOLO INFO RELEVANTE]

Requisitos:
- Seguir convenciones del proyecto
- Incluir tests
- Documentar cambios
```

**Por cada tarea:**
1. Nueva conversaciÃ³n (balde limpio)
2. Contexto mÃ­nimo relevante
3. ImplementaciÃ³n
4. ValidaciÃ³n
5. Commit

## ğŸ”„ Workflow completo

### Ejemplo: Feature "Sistema de notificaciones"

#### ğŸ“‹ Fase 1: InvestigaciÃ³n

```bash
# 1. ConversaciÃ³n limpia
[Nueva conversaciÃ³n]

# 2. Prompt al Investigador
"ActuÃ¡ como experto en notificaciones push.

InvestigÃ¡ para mi app:
1. Â¿QuÃ© servicios existen? (Firebase, OneSignal, etc.)
2. Â¿QuÃ© tenemos implementado?
3. Â¿CuÃ¡l recomendÃ¡s y por quÃ©?
4. Â¿Complejidad y costos?

Genera resumen estructurado."

# 3. Output del modelo
[Documento de investigaciÃ³n]

# 4. Revisar con Regla de los 5
âœ… Borrador: Completo (3 servicios evaluados)
âš ï¸  CorrecciÃ³n: OneSignal pricing desactualizado
âœ… Claridad: Se entiende bien
âš ï¸  Casos lÃ­mite: Falta considerar iOS vs Android
âœ… Excelencia: RecomendaciÃ³n justificada

# 5. Refinar y aprobar
[Documento investigaciÃ³n FINAL]
```

#### ğŸ“ Fase 2: PlanificaciÃ³n

```bash
# 1. NUEVA conversaciÃ³n (balde limpio)
[Nueva conversaciÃ³n]

# 2. Prompt a la Estratega
"Con base en esta investigaciÃ³n:
[PEGAR RESUMEN APROBADO]

Genera plan paso a paso.
Requisito: cada tarea < 4h, lo mÃ¡s pequeÃ±a posible."

# 3. Output del modelo
Plan con 8 tareas pequeÃ±as

# 4. Revisar con Regla de los 5
âœ… Borrador: 8 tareas definidas
âœ… CorrecciÃ³n: LÃ³gica correcta
âš ï¸  Claridad: Tarea 3 poco clara â†’ refinar
âœ… Casos lÃ­mite: Certificados iOS considerados
âš ï¸  Excelencia: Falta estimaciÃ³n â†’ agregar

# 5. Plan FINAL aprobado
```

#### âš¡ Fase 3: EjecuciÃ³n

```bash
# Por cada tarea (8 conversaciones)

# Tarea 1: Setup Firebase
[Nueva conversaciÃ³n]
Contexto: Plan + tarea 1
â†’ Implementar
â†’ Validar
â†’ Commit: "feat(notifications): setup Firebase"

# Tarea 2: Backend endpoints
[Nueva conversaciÃ³n]
Contexto: Plan + tarea 2
â†’ Implementar
â†’ Validar
â†’ Commit: "feat(notifications): add send endpoint"

# ... Tarea 3-8 ...
```

**Cada tarea:**
- Balde limpio (nueva conversaciÃ³n)
- Contexto mÃ­nimo
- Regla de los 5 aplicada
- Commit individual

## ğŸ¨ El arte de preguntar

La calidad de la respuesta depende de la calidad de la pregunta.

### AnatomÃ­a de una buena pregunta

```
âŒ Pregunta vaga:
"Ayudame con notificaciones"

âœ… Pregunta especÃ­fica:
"Necesito implementar notificaciones push para app mÃ³vil (iOS + Android).

Contexto:
- Ya tenemos Firebase Analytics
- 10K usuarios activos
- Necesitamos: eventos en tiempo real + marketing

Dame recomendaciÃ³n justificada entre:
- Firebase Cloud Messaging
- OneSignal
- Pusher

ConsiderÃ¡: costo, complejidad setup, features."
```

### Template SBAR (Situation-Background-Assessment-Recommendation)

```markdown
**Situation** (SituaciÃ³n actual)
Necesito [OBJETIVO]

**Background** (Contexto relevante)
- Proyecto: [TIPO]
- Stack actual: [TECNOLOGÃAS]
- Restricciones: [LÃMITES]

**Assessment** (Lo que sÃ©/intentÃ©)
- Ya investiguÃ©: [X, Y]
- ProbÃ©: [Z] pero [PROBLEMA]

**Recommendation** (QuÃ© necesito)
Dame [TIPO DE OUTPUT] que incluya [REQUISITOS]
```

### Ejemplo real SBAR

```markdown
**Situation**
Necesito agregar cachÃ© a API REST

**Background**
- API Node.js + Express
- 50 requests/sec promedio
- Redis ya configurado (analytics)
- Endpoints mÃ¡s lentos: /users, /posts

**Assessment**
- SÃ© que necesito TTL diferentes por endpoint
- ProbÃ© cache-manager pero config compleja
- Middleware approach parece mejor

**Recommendation**
Dame implementaciÃ³n de:
1. Middleware de cachÃ© con Redis
2. ConfiguraciÃ³n por endpoint (TTL)
3. InvalidaciÃ³n en POST/PUT/DELETE
4. Tests de integraciÃ³n
```

## ğŸ¯ Checklist de uso

Antes de aceptar un output del modelo:

```
ğŸ“‹ Regla de los 5:
â–¡ Â¿Es completo? (Borrador)
â–¡ Â¿Es correcto? (CorrecciÃ³n)
â–¡ Â¿Se entiende? (Claridad)
â–¡ Â¿QuÃ© podrÃ­a fallar? (Casos lÃ­mite)
â–¡ Â¿Es lo mejor posible? (Excelencia)

ğŸª£ Baldes limpios:
â–¡ Â¿ConversaciÃ³n enfocada en UN tema?
â–¡ Â¿Contexto relevante SOLO para esta tarea?
â–¡ Â¿Sin info contradictoria/obsoleta?

ğŸ‘¥ Tres expertos:
â–¡ Â¿InvestigaciÃ³n revisada por humano?
â–¡ Â¿Plan con tareas < 4h?
â–¡ Â¿Una conversaciÃ³n por tarea de ejecuciÃ³n?

ğŸ“Š OptimizaciÃ³n:
â–¡ Â¿Pregunta bien formulada?
â–¡ Â¿Contexto mÃ­nimo necesario?
â–¡ Â¿Output reutilizable como plantilla?
```

## ğŸ’¡ Tips prÃ¡cticos

### Reutilizar outputs como plantillas

Cada vez que obtienes un output bueno (pasÃ³ Regla de los 5), guÃ¡rdalo:

```bash
# Ejemplo: Plan de migraciÃ³n de DB quedÃ³ excelente
cp planificacion-migracion.md templates/workflows/migracion-db.md

# PrÃ³xima vez:
# Solo cambiar variables especÃ­ficas del proyecto
```

### Mantener biblioteca de prompts

```
prompts/
â”œâ”€â”€ investigacion-tecnologia.md
â”œâ”€â”€ planificacion-feature.md
â”œâ”€â”€ debug-error.md
â””â”€â”€ refactor-codigo.md
```

### Nombrar conversaciones

Cuando trabajes en herramienta con mÃºltiples conversaciones:

```
Investigador: [PROYECTO] - InvestigaciÃ³n
Estratega: [PROYECTO] - Plan
Ejecutor T1: [PROYECTO] - Tarea 1
Ejecutor T2: [PROYECTO] - Tarea 2
...
```

Ayuda a mantener claro quÃ© balde es cuÃ¡l.

## âš ï¸ Trampas comunes

### 1. "ConversaciÃ³n Ãºnica para todo"

```
âŒ Malo:
[ConversaciÃ³n con 50 mensajes]
- InvestigaciÃ³n
- PlanificaciÃ³n
- EjecuciÃ³n
- Debugging
- Refactor
â†’ Balde SUCIO, outputs confusos

âœ… Bueno:
[5 conversaciones separadas]
Cada una con propÃ³sito claro
â†’ Baldes LIMPIOS, outputs excelentes
```

### 2. "Aceptar primera respuesta"

```
âŒ Malo:
Prompt â†’ Output â†’ Usar directamente

âœ… Bueno:
Prompt â†’ Output â†’ Regla de los 5 â†’ Refinar â†’ Usar
```

### 3. "Tareas muy grandes"

```
âŒ Malo:
Tarea: "Implementar todo el sistema de usuarios"
â†’ No cabe en un balde

âœ… Bueno:
Tarea 1: "Schema de users en DB"
Tarea 2: "Endpoint POST /register"
Tarea 3: "Endpoint POST /login"
â†’ Cada una cabe en un balde
```

### 4. "Contexto excesivo"

```
âŒ Malo:
[Pegar todo el cÃ³digo del proyecto]

âœ… Bueno:
[Pegar SOLO archivos relevantes para esta tarea]
```

### 5. "No revisar conocimiento implÃ­cito"

```
âŒ Malo:
InvestigaciÃ³n: "Firebase es lo mejor"
Aceptar sin revisar

âœ… Bueno:
InvestigaciÃ³n: "Firebase es lo mejor"
Revisar: "Pero nosotros ya tenemos Pusher configurado"
â†’ DecisiÃ³n informada
```

## ğŸ“š Referencias

- Post original de Resonant Coding: https://charly-vibes.github.io/microdancing/es/posts/resonant-coding
- Context Engineering: https://github.com/humanlayer/advanced-context-engineering-for-coding-agents
- Regla de los 5 (Steve Yegge): https://github.com/steveyegge/gastown/blob/main/internal/formula/formulas/rule-of-five.formula.toml

---

**Meta:** Esta skill fue creada usando la metodologÃ­a que describe. ğŸ•
